{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read an image for visually challenged","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n* [Problem Statement](#section-1)\n* [Importing libraries and reading data](#section-2)\n* [Data Understanding](#section-3)\n* [Pre-processing the captions](#section-4)\n* [Pre-processing the images](#section-5)\n* [Extracting the Feature vector](#section-6)\n* [Model Building](#section-7)\n    - [Data preperation](#subsect-1)\n    - [Build the Encoder](#subsect-2)\n    - [Build the Attention model](#subsect-3)\n    - [Build the Decoder](#subsect-4)\n* [Model training & optimization](#section-8)\n* [Model Evaluation](#section-9)\n* [Conclusions](#section-10)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-1\"></a>\n# Problem Statement","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n    The World Health Organization (WHO) has reported that approximately 285 million people are visually impaired worldwide, and out of these 285 million, 39 million are completely blind. It gets extremely tough for them to carry out daily activities, one of which is reading. From reading a newspaper or a magazine to reading an important text message from your bank, it is tough for them to read the text written in it.\n\n    A similar problem they also face is seeing and enjoying the beauty of pictures and images. Today, in the world of social media, millions of images are uploaded daily. Some of them are about your friends and family, while some of them are about nature and its beauty. Understanding what is present in that image is quite a challenge for certain people who are suffering from visual impairment or who are blind.\n\n    In an initiative to help them experience the beauty of the images, Facebook had earlier launched a unique feature earlier that can help blind people operate the app on their mobile phones. The feature could explain the contents of an image that their friends have posted on Facebook. So, say, if someone posted a picture with their dog in the park, the application would speak out the contents and may describe it like, “This image may contain a dog standing with a man around the trees.”\n    \n    In this project, the intention is to build a model that is similar to the one developed by Facebook so that a visually challenged person knows the contents of an image in the form of audio. \n    Flickr8K dataset is used in the project, the dataset is available in Kaggle.\n    \n## Approach\n    Firstly the information in an image is converted into a text description, then using a text to speech API, the audio output is extracted from the text description.\n    \n    This problem statement is an application of both deep learning and natural language processing. The features of an image will be extracted by a CNN-based encoder and this will be decoded by an RNN model. To generate a more meaningful and descriptive caption\n    The main important portion is to generate the text description. A deep learning model using a CNN-RNN model with an attention mechanism is employed to generate the caption ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-2\"></a>\n# Importing libraries and reading data","metadata":{}},{"cell_type":"markdown","source":"#### Mandatory codeblock to facilitate running on Kaggle","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:49:23.832452Z","iopub.execute_input":"2022-04-08T17:49:23.833109Z","iopub.status.idle":"2022-04-08T17:49:32.536978Z","shell.execute_reply.started":"2022-04-08T17:49:23.833006Z","shell.execute_reply":"2022-04-08T17:49:32.536315Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"'''\ngTTS - (Google Text-to-Speech)is a Python library and CLI tool to interface with Google Translate text-to-speech API. \n\nplaysound - The playsound module is a cross platform module that can play audio files. \n'''\n!pip install gtts\n!pip install playsound\n!pip install ipython","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:49:37.522951Z","iopub.execute_input":"2022-04-08T17:49:37.523606Z","iopub.status.idle":"2022-04-08T17:50:04.286727Z","shell.execute_reply.started":"2022-04-08T17:49:37.523567Z","shell.execute_reply":"2022-04-08T17:50:04.285820Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Import all the required libraries\n\nimport os                   # Pathname manipulation\nimport glob                 # For pattern matching of pathnames\nimport time                 # Provides functions related to time values\nimport random               # To generate random numbers\nimport re                   # Regular expressions\nfrom tqdm import tqdm       # Used for creating Progress Meters or Progress Bars ('taqaddum' in arabic for progress)\n# Module provides additional data structures for collections of data like defaultdict, ordereddict, counter, Deque, Namedtuple, Chainmap\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\n\n# PIL - Python Imaging Library, adds many image processing features\nfrom PIL import Image\nfrom skimage import io\n\nfrom sklearn.model_selection import train_test_split \n\n#model building \nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.translate.bleu_score import sentence_bleu\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import image, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop \nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.models import Model\nfrom keras import Input, layers\nfrom keras import optimizers\n\nfrom gtts import gTTS\nfrom playsound import playsound\nfrom IPython import display","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:22.062627Z","iopub.execute_input":"2022-04-08T17:50:22.063475Z","iopub.status.idle":"2022-04-08T17:50:28.641981Z","shell.execute_reply.started":"2022-04-08T17:50:22.063437Z","shell.execute_reply":"2022-04-08T17:50:28.641229Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-3\"></a>\n# Data Understanding\n1.Importing the dataset and reading image & captions into two seperate variables\n\n2.Visualising both the images & text present in the dataset\n\n3.Creating a dataframe which summarizes the image, path & captions as a dataframe\n\n4.Creating a list which contains all the captions & path\n\n5.Visualising the top 30 occuring words in the captions\n","metadata":{}},{"cell_type":"code","source":"#Importing the dataset and reading the image into a seperate variable\n\nimages_folder_path = '/kaggle/input/flickr8k/Images'  \n# alternative code> images_folder_path = os.path.join('/','kaggle','input','flickr8k','Images')\n\ncaptions_file_path = os.path.join('/','kaggle','input','flickr8k','captions.txt')\n# alternative code> captions_file_path = '/kaggle/input/flickr8k/captions.txt'\n\nworking_folder_path = os.path.join('/','kaggle','working')\n\n# Creating a list which contains the path to the images\nall_imgs_path_list = glob.glob(images_folder_path + '/*.jpg',recursive=True) \nprint(\"The total images present in the dataset: \", len(all_imgs_path_list))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:39.479561Z","iopub.execute_input":"2022-04-08T17:50:39.479860Z","iopub.status.idle":"2022-04-08T17:50:39.518826Z","shell.execute_reply.started":"2022-04-08T17:50:39.479825Z","shell.execute_reply":"2022-04-08T17:50:39.517889Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Visualising both the images & text present in the dataset\n\n# Visualizing a random image\n# Random image index\nrandom_img_index = np.random.randint(low=0, high=len(all_imgs_path_list)-1, size=None, dtype=int)\nImage.open(all_imgs_path_list[random_img_index])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:43.731302Z","iopub.execute_input":"2022-04-08T17:50:43.731564Z","iopub.status.idle":"2022-04-08T17:50:43.831200Z","shell.execute_reply.started":"2022-04-08T17:50:43.731533Z","shell.execute_reply":"2022-04-08T17:50:43.830573Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset and read the text file into a seperate variable\n\ndef load_doc(filename):\n    \n    text=open(filename).read()\n    return text\n\ndoc = load_doc(captions_file_path)\nprint(doc[:500])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:48.413168Z","iopub.execute_input":"2022-04-08T17:50:48.413647Z","iopub.status.idle":"2022-04-08T17:50:48.472298Z","shell.execute_reply.started":"2022-04-08T17:50:48.413609Z","shell.execute_reply":"2022-04-08T17:50:48.471583Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"captions_df = pd.read_csv(captions_file_path) # Creating a captions dataframe with comma seperated values of images & captions \n\nprint(\"Total captions present in the dataset: \", len(list(captions_df.caption)))\nprint(\"Total images present in the dataset: \",len(all_imgs_path_list))\n\ncaptions_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:50.572982Z","iopub.execute_input":"2022-04-08T17:50:50.573541Z","iopub.status.idle":"2022-04-08T17:50:50.644619Z","shell.execute_reply.started":"2022-04-08T17:50:50.573501Z","shell.execute_reply":"2022-04-08T17:50:50.643802Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Each image id has 5 captions associated with it therefore the total dataset should have 8091x5 = 40455 samples.","metadata":{}},{"cell_type":"code","source":"all_img_id = list(captions_df.image)    # store all the image id here as list\nall_img_vector = all_imgs_path_list     # store all the image path here as list\nannotations = list(captions_df.caption) # store all the captions here as list\n\ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['Image-ID','Path', 'Captions']) \n# df is dataframe containing image-id, image path and caption    \ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:53.811625Z","iopub.execute_input":"2022-04-08T17:50:53.812365Z","iopub.status.idle":"2022-04-08T17:50:53.844396Z","shell.execute_reply.started":"2022-04-08T17:50:53.812323Z","shell.execute_reply":"2022-04-08T17:50:53.843629Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Adding the <start> & <end> token to all those captions as well\n\nstart_token = '<start>'\nend_token = '<end>'\n\nimage_path_to_caption = defaultdict(list) # Dictionary with key: image id, values: list of 5 captions \n\nfor idx, row in captions_df.iterrows():\n    caption = f'{start_token} {row.caption} {end_token}' # adding <start> & <end> token for each caption\n    image_path = os.path.join(images_folder_path,row.image) # image_path contains list of image paths\n    image_path_to_caption[image_path].append(caption) # Generates dictionary of image_path: caption","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:50:56.607598Z","iopub.execute_input":"2022-04-08T17:50:56.608143Z","iopub.status.idle":"2022-04-08T17:50:58.951836Z","shell.execute_reply.started":"2022-04-08T17:50:56.608105Z","shell.execute_reply":"2022-04-08T17:50:58.951105Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# image_path_to_caption ","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:03.082527Z","iopub.execute_input":"2022-04-08T17:51:03.083090Z","iopub.status.idle":"2022-04-08T17:51:03.204087Z","shell.execute_reply.started":"2022-04-08T17:51:03.083048Z","shell.execute_reply":"2022-04-08T17:51:03.203281Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Visualizing a random image and its captions\n\nimage_num = np.random.randint(low=0, high=len(all_imgs_path_list)-1, size=None, dtype=int)\nprint(list(image_path_to_caption.values())[image_num])\nImage.open(list(image_path_to_caption.keys())[image_num])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:13.696185Z","iopub.execute_input":"2022-04-08T17:51:13.696475Z","iopub.status.idle":"2022-04-08T17:51:13.802228Z","shell.execute_reply.started":"2022-04-08T17:51:13.696443Z","shell.execute_reply":"2022-04-08T17:51:13.801414Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Visualizing image and corresponding captions\n\nimg_path_from_dict = list(image_path_to_caption.keys())\nall_captions = []  # Caption list containing captions with start & end token for all the images\nimg_name_vector = [] # Creating image path list (ordered based on caption sets of 5, captions with start & end token)\n\nfor img_path in img_path_from_dict:\n    \n    # Caption list containing 5 captions with start & end token corresponding to a single image\n    caption_list = image_path_to_caption[img_path] \n    \n    # Caption list containing captions with start & end token for all images\n    all_captions.extend(caption_list)\n    \n    # Creating image path list (ordered based on caption sets of 5, captions with start & end token)\n    img_name_vector.extend([img_path] * len(caption_list))\n\n# random image index \nimage_num = np.random.randint(low=0, high=len(all_imgs_path_list)-1, size=None, dtype=int)\nimage_num = image_num//5\nimage_num = image_num*5\n\nf, axes = plt.subplots(1, 2)\nplt.axis('off')\nf.set_figwidth(10)\naxes[0].imshow(io.imread(img_name_vector[image_num]))\naxes[0].axis('off')\naxes[1].set_ylim(0,5)\nfor i in range(0,5):\n    axes[1].text(0,i,all_captions[i+image_num])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:20.004045Z","iopub.execute_input":"2022-04-08T17:51:20.004929Z","iopub.status.idle":"2022-04-08T17:51:20.264996Z","shell.execute_reply.started":"2022-04-08T17:51:20.004886Z","shell.execute_reply":"2022-04-08T17:51:20.264314Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Creating the vocabulary & the counter for the captions\n\nvocabulary= [word.lower() for line in annotations for word in line.split()]\n\nval_count=Counter(vocabulary)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:24.797807Z","iopub.execute_input":"2022-04-08T17:51:24.798433Z","iopub.status.idle":"2022-04-08T17:51:24.940861Z","shell.execute_reply.started":"2022-04-08T17:51:24.798397Z","shell.execute_reply":"2022-04-08T17:51:24.940139Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# val_count","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:30.201946Z","iopub.execute_input":"2022-04-08T17:51:30.202620Z","iopub.status.idle":"2022-04-08T17:51:30.239253Z","shell.execute_reply.started":"2022-04-08T17:51:30.202584Z","shell.execute_reply":"2022-04-08T17:51:30.238615Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Visualising the top 30 occuring words in the captions\n\ntop30_words_countwise = val_count.most_common(30)\ntop30_words_df = pd.DataFrame(top30_words_countwise, columns = ['Word', 'Count'])\nprint(top30_words_df)\n\nfig = plt.figure(figsize=(14,8))\nsns.barplot(x='Word', y='Count', data = top30_words_df)\nplt.title(\"Top 30 maximum frequency words\", fontsize = 18)\nplt.xlabel(\"Words\", fontsize = 14)\nplt.ylabel(\"Count\", fontsize = 14)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:37.489604Z","iopub.execute_input":"2022-04-08T17:51:37.490149Z","iopub.status.idle":"2022-04-08T17:51:37.900181Z","shell.execute_reply.started":"2022-04-08T17:51:37.490113Z","shell.execute_reply":"2022-04-08T17:51:37.899488Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-4\"></a>\n# Pre-processing the captions\n1.Creating the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives a vocabulary of all of the unique words in the data. The total vocaublary is set to top 5,000 words for saving memory.\n\n2.Replacing all other words with the unknown token \"UNK\" .\n\n3.Creating word-to-index and index-to-word mappings.\n\n4.Padding all sequences to be the same length as the longest one.","metadata":{}},{"cell_type":"code","source":"# creating the tokenizer\n\n# Top 5000 words\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n# annotations =  list of all captions \n# fit_on_texts >> Updates internal vocabulary based on a list of texts, in case of texts containing lists, each entry of the lists is assumed to be a token.\n# Required before using `texts_to_sequences`\ntokenizer.fit_on_texts(all_captions) \n# Transforms each text in texts to a sequence of integers\ntokenized_text_sequence = tokenizer.texts_to_sequences(all_captions)\ntokenized_text_sequence[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:44.063669Z","iopub.execute_input":"2022-04-08T17:51:44.064239Z","iopub.status.idle":"2022-04-08T17:51:45.385361Z","shell.execute_reply.started":"2022-04-08T17:51:44.064198Z","shell.execute_reply":"2022-04-08T17:51:45.384610Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Creating word-to-index and index-to-word mappings.\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\ntokenized_text_sequence = tokenizer.texts_to_sequences(all_captions)\ntokenized_text_sequence[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:48.868141Z","iopub.execute_input":"2022-04-08T17:51:48.868719Z","iopub.status.idle":"2022-04-08T17:51:49.389963Z","shell.execute_reply.started":"2022-04-08T17:51:48.868679Z","shell.execute_reply":"2022-04-08T17:51:49.389048Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"random_index = np.random.randint(low=0, high=len(all_imgs_path_list)-1, size=None, dtype=int)\nprint(all_captions[random_index])\nprint(tokenized_text_sequence[random_index])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:54.618907Z","iopub.execute_input":"2022-04-08T17:51:54.619182Z","iopub.status.idle":"2022-04-08T17:51:54.625355Z","shell.execute_reply.started":"2022-04-08T17:51:54.619152Z","shell.execute_reply":"2022-04-08T17:51:54.624662Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Creating a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n\ndict_word_count = tokenizer.word_counts","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:51:56.378866Z","iopub.execute_input":"2022-04-08T17:51:56.379665Z","iopub.status.idle":"2022-04-08T17:51:56.385020Z","shell.execute_reply.started":"2022-04-08T17:51:56.379625Z","shell.execute_reply":"2022-04-08T17:51:56.384018Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# dict_word_count","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:03.473449Z","iopub.execute_input":"2022-04-08T17:52:03.473711Z","iopub.status.idle":"2022-04-08T17:52:03.528657Z","shell.execute_reply.started":"2022-04-08T17:52:03.473680Z","shell.execute_reply":"2022-04-08T17:52:03.527949Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Visualising the top 30 occuring words in the captions after text processing\n\ntokenized_word_count = sorted(tokenizer.word_counts.items(), key=lambda x:x[1], reverse=True)\ntokenized_word_count_df = pd.DataFrame(tokenized_word_count, columns = ['Word', 'Count'])\ntop30_tokenized_words_df = tokenized_word_count_df.iloc[0:30,:]\nprint(top30_tokenized_words_df)\n\nfig = plt.figure(figsize=(20,10))\nsns.barplot(x='Word', y='Count', data = top30_tokenized_words_df)\nplt.title(\"Top 30 maximum frequency words\", fontsize = 18)\nplt.xlabel(\"Words\", fontsize = 14)\nplt.ylabel(\"Count\", fontsize = 14)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:11.092933Z","iopub.execute_input":"2022-04-08T17:52:11.093448Z","iopub.status.idle":"2022-04-08T17:52:11.503040Z","shell.execute_reply.started":"2022-04-08T17:52:11.093410Z","shell.execute_reply":"2022-04-08T17:52:11.502347Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# all_captions - all captions list including start & end token\nall_tokenized_text_sequence = tokenizer.texts_to_sequences(all_captions) \n\nmax_length_caption = max(len(i) for i in all_tokenized_text_sequence)\nprint('Maximum length of any caption in the dataset: ', max_length_caption)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:20.095743Z","iopub.execute_input":"2022-04-08T17:52:20.096505Z","iopub.status.idle":"2022-04-08T17:52:20.644931Z","shell.execute_reply.started":"2022-04-08T17:52:20.096463Z","shell.execute_reply":"2022-04-08T17:52:20.644133Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Padding each vector to the max_length of the captions ^ store it to a vairable\n\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(all_tokenized_text_sequence, padding = 'post',maxlen = max_length_caption)\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))\ncap_vector[1:3,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:29.318320Z","iopub.execute_input":"2022-04-08T17:52:29.318989Z","iopub.status.idle":"2022-04-08T17:52:29.532702Z","shell.execute_reply.started":"2022-04-08T17:52:29.318952Z","shell.execute_reply":"2022-04-08T17:52:29.531990Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-5\"></a>\n# Pre-processing the images\n\n* Resizing the image to the shape of (299, 299)\n* Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3.\n#### Note on resizing the images:\n* Since the list contains all the image path, we first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices</i>. Once the dataset is created, consisting of image paths, we apply a function to the dataset which will apply the necessary preprocessing to each image. \n* This function would resize them and also do the necessary preprocessing correct format of InceptionV3.","metadata":{}},{"cell_type":"code","source":"# Creating the function which returns images & their path\n\ndef load_image(image_path):\n    #write your pre-processing steps here\n    preprocessed_img = tf.io.read_file(image_path)\n    preprocessed_img = tf.image.decode_jpeg(preprocessed_img, channels=3)\n    preprocessed_img = tf.image.resize(preprocessed_img, (299, 299))\n    # tf.keras.applications.inception_v3.preprocess_input will normalize input to range of -1 to 1\n    preprocessed_img = tf.keras.applications.inception_v3.preprocess_input(preprocessed_img)\n    return preprocessed_img, image_path","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:32.857943Z","iopub.execute_input":"2022-04-08T17:52:32.858478Z","iopub.status.idle":"2022-04-08T17:52:32.867069Z","shell.execute_reply.started":"2022-04-08T17:52:32.858433Z","shell.execute_reply":"2022-04-08T17:52:32.866234Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Applying the function to the image path dataset, such that the transformed dataset would contain images & their path\n\nencode_train_set = sorted(set(img_name_vector))    # img_name_vector - path list of all images according to caption set with start & end token\n\nimage_data_set = tf.data.Dataset.from_tensor_slices(encode_train_set)\nimage_data_set = image_data_set.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\nimage_data_set\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:37.436746Z","iopub.execute_input":"2022-04-08T17:52:37.437453Z","iopub.status.idle":"2022-04-08T17:52:39.923392Z","shell.execute_reply.started":"2022-04-08T17:52:37.437415Z","shell.execute_reply":"2022-04-08T17:52:39.922611Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-6\"></a>\n# Extracting the Feature vector\n#### Loading the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, we extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Using a function to extract the features of each image in the train & test dataset such that the shape of each image would be (batch_size, 8*8, 2048)\n##### Note on storing the features:\n* We can store the features using a dictionary with the path as the key and the feature extracted by the inception net v3 model as values. OR\n* We can store using numpy(np.save) to store the resulting vector.","metadata":{}},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\nnew_input = image_model.input                    #write code here to get the input of the image_model\nhidden_layer = image_model.layers[-1].output     #write code here to get the output of the image_model\n\nimage_features_extract_model = keras.Model(new_input, hidden_layer)   #build the final model using both input & output layer","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:46.724420Z","iopub.execute_input":"2022-04-08T17:52:46.724687Z","iopub.status.idle":"2022-04-08T17:52:49.131390Z","shell.execute_reply.started":"2022-04-08T17:52:46.724655Z","shell.execute_reply":"2022-04-08T17:52:49.130704Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"image_features_extract_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:52.191783Z","iopub.execute_input":"2022-04-08T17:52:52.192065Z","iopub.status.idle":"2022-04-08T17:52:52.332491Z","shell.execute_reply.started":"2022-04-08T17:52:52.192028Z","shell.execute_reply":"2022-04-08T17:52:52.331803Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Applying the feature_extraction model to earlier created dataset which contained images & their respective paths\n\nfeature_dict = {}\n\n# using tqdm for progress bar\nfor image,path in tqdm(image_data_set):  \n    \n    # feed images from newly created Dataset above to Inception V3 built above    \n    batch_features = image_features_extract_model(image)\n    \n    # To squeeze out the features in a batch, to reshape features in order of (batch_size, 8*8, 2048)    \n    batch_features_reshaped = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n    \n    for batch_f, p in zip(batch_features_reshaped, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        feature_dict[path_of_feature] =  batch_f.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:52:58.154466Z","iopub.execute_input":"2022-04-08T17:52:58.154722Z","iopub.status.idle":"2022-04-08T17:54:20.131614Z","shell.execute_reply.started":"2022-04-08T17:52:58.154691Z","shell.execute_reply":"2022-04-08T17:54:20.130830Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"image_data_set","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:25.691797Z","iopub.execute_input":"2022-04-08T17:54:25.692364Z","iopub.status.idle":"2022-04-08T17:54:25.699768Z","shell.execute_reply.started":"2022-04-08T17:54:25.692327Z","shell.execute_reply":"2022-04-08T17:54:25.698910Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# batch_features","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:32.786677Z","iopub.execute_input":"2022-04-08T17:54:32.787388Z","iopub.status.idle":"2022-04-08T17:54:32.820077Z","shell.execute_reply.started":"2022-04-08T17:54:32.787350Z","shell.execute_reply":"2022-04-08T17:54:32.819244Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# batch_features_reshaped","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:41.715895Z","iopub.execute_input":"2022-04-08T17:54:41.716167Z","iopub.status.idle":"2022-04-08T17:54:41.736475Z","shell.execute_reply.started":"2022-04-08T17:54:41.716137Z","shell.execute_reply":"2022-04-08T17:54:41.735796Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print('Shape of batch_features - ',batch_features.shape)\nprint('Shape of batch_features_reshaped - ',batch_features_reshaped.shape)\nprint('Shape of feature extracted for an image',batch_f.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:48.896312Z","iopub.execute_input":"2022-04-08T17:54:48.896569Z","iopub.status.idle":"2022-04-08T17:54:48.901923Z","shell.execute_reply.started":"2022-04-08T17:54:48.896536Z","shell.execute_reply":"2022-04-08T17:54:48.901071Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-7\"></a>\n# Model Building","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsect-1\"></a>\n# Data preperation\n1. Applying train_test_split on both image path & captions to create the train & test list. Creating the train-test spliit using 80-20 ratio & random state = 42\n2. Creating a function which maps the image path to their feature. \n3. Creating a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n4. Shuffle the batch while building the dataset\n5. The shape of each image in the dataset after building would be (batch_size, 8*8, 2048)\n6. The shape of each caption in the dataset after building would be(batch_size, max_len)\n\n#### Note on loading the features:\n* We can load the features using the dictionary created earlier. OR\n* We can store using numpy(np.load) to load the feature vector.","metadata":{}},{"cell_type":"code","source":"# Applying train_test_split on both image path & captions to create the train & test list. \n# Creating the train-test spliit using 80-20 ratio & random state = 42\n\npath_train, path_test, cap_train, cap_test = train_test_split(img_name_vector, cap_vector, test_size=0.2, random_state=42)\nprint(\"Training data for images: \" + str(len(path_train)))\nprint(\"Testing data for images: \" + str(len(path_test)))\nprint(\"Training data for Captions: \" + str(len(cap_train)))\nprint(\"Testing data for Captions: \" + str(len(cap_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:54.795134Z","iopub.execute_input":"2022-04-08T17:54:54.795621Z","iopub.status.idle":"2022-04-08T17:54:54.821262Z","shell.execute_reply.started":"2022-04-08T17:54:54.795582Z","shell.execute_reply":"2022-04-08T17:54:54.820397Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Creating a function which maps the image path to their feature. \n# This function will take the image_path & caption and return it's feature & respective caption.\n\ndef map_func(image_name,capt):     \n  image_tensor = feature_dict[image_name.decode('utf-8')]    # Code to extract the features from the dictionary stored earlier\n  return image_tensor,capt","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:54:59.077939Z","iopub.execute_input":"2022-04-08T17:54:59.078627Z","iopub.status.idle":"2022-04-08T17:54:59.082792Z","shell.execute_reply.started":"2022-04-08T17:54:59.078589Z","shell.execute_reply":"2022-04-08T17:54:59.082029Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# creating a builder function to create dataset which takes in the image path & captions as input\n# This function would transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n\nBATCH_SIZE = 32\nBUFFER_SIZE = 1000\ndef gen_dataset(images_data, captions_data):\n        \n    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n    dataset = dataset.shuffle(BUFFER_SIZE)\n \n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-08T17:55:02.064224Z","iopub.execute_input":"2022-04-08T17:55:02.064507Z","iopub.status.idle":"2022-04-08T17:55:02.070866Z","shell.execute_reply.started":"2022-04-08T17:55:02.064476Z","shell.execute_reply":"2022-04-08T17:55:02.070138Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_dataset=gen_dataset(path_train,cap_train)\ntest_dataset=gen_dataset(path_test,cap_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:04.966713Z","iopub.execute_input":"2022-04-08T17:55:04.967287Z","iopub.status.idle":"2022-04-08T17:55:05.148606Z","shell.execute_reply.started":"2022-04-08T17:55:04.967249Z","shell.execute_reply":"2022-04-08T17:55:05.147905Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\nprint(sample_cap_batch.shape) #(batch_size,max_len)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:07.889778Z","iopub.execute_input":"2022-04-08T17:55:07.890513Z","iopub.status.idle":"2022-04-08T17:55:07.930290Z","shell.execute_reply.started":"2022-04-08T17:55:07.890472Z","shell.execute_reply":"2022-04-08T17:55:07.929442Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Setting the parameters\n\nembedding_dim = 256 \nunits = 512\nvocab_size = 5001   #top 5,000 words +1\ntrain_num_steps = len(path_train) // BATCH_SIZE\ntest_num_steps = len(path_test) // BATCH_SIZE\n\nfeatures_shape = batch_f.shape[1]\nattention_features_shape = batch_f.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:11.444198Z","iopub.execute_input":"2022-04-08T17:55:11.444449Z","iopub.status.idle":"2022-04-08T17:55:11.449217Z","shell.execute_reply.started":"2022-04-08T17:55:11.444420Z","shell.execute_reply":"2022-04-08T17:55:11.448519Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsect-2\"></a>\n# Building the Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self,embedding_dim):\n        super(Encoder, self).__init__()\n        self.dense = layers.Dense(embedding_dim)       #build your Dense layer with relu activation\n        \n    def call(self, features, training=False):\n        # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features = self.dense(features)\n        features = tf.nn.relu(features)\n        return features","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:15.952326Z","iopub.execute_input":"2022-04-08T17:55:15.952868Z","iopub.status.idle":"2022-04-08T17:55:15.959054Z","shell.execute_reply.started":"2022-04-08T17:55:15.952825Z","shell.execute_reply":"2022-04-08T17:55:15.957963Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"encoder=Encoder(embedding_dim)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:18.482621Z","iopub.execute_input":"2022-04-08T17:55:18.483225Z","iopub.status.idle":"2022-04-08T17:55:18.494082Z","shell.execute_reply.started":"2022-04-08T17:55:18.483184Z","shell.execute_reply":"2022-04-08T17:55:18.493357Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsect-3\"></a>\n# Building the Attention model","metadata":{}},{"cell_type":"code","source":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = layers.Dense(units)       #build Dense layer\n        self.W2 = layers.Dense(units)       #build Dense layer\n        self.V = layers.Dense(1)            #build final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        \n        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        hidden_with_time_axis =  tf.expand_dims(hidden, 1)  \n        \n        # build the score funciton to shape: (batch_size, 8*8, units)\n        score = keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) \n        \n        # extract the attention weights with shape: (batch_size, 8*8, 1)\n        attention_weights = keras.activations.softmax(self.V(score), axis=1)\n        \n        #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = attention_weights * features \n        \n        # reduce the shape to (batch_size, embedding_dim)\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n    \n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:20.858214Z","iopub.execute_input":"2022-04-08T17:55:20.858828Z","iopub.status.idle":"2022-04-08T17:55:20.867514Z","shell.execute_reply.started":"2022-04-08T17:55:20.858785Z","shell.execute_reply":"2022-04-08T17:55:20.866713Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsect-4\"></a>\n# Building the Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units)                # iniitalise the Attention model with units\n        self.embed = layers.Embedding(vocab_size, embedding_dim)    # build the Embedding layer     \n        self.gru = layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n        self.d1 = layers.Dense(self.units)              # build the Dense layer    \n        self.d2 = layers.Dense(vocab_size)              # build the Dense layer\n        \n    def call(self,x,features, hidden):\n        #create the context vector & attention weights from attention model\n        context_vector, attention_weights = self.attention(features, hidden)\n        # embed the input to shape: (batch_size, 1, embedding_dim)\n        embed = self.embed(x) \n        # Concatenate the input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) \n        # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        output,state = self.gru(embed)\n        output = self.d1(output)\n        # shape : (batch_size * max_length, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2])) \n        # shape : (batch_size * max_length, vocab_size)\n        output = self.d2(output) \n        \n        return output, state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:25.604355Z","iopub.execute_input":"2022-04-08T17:55:25.605122Z","iopub.status.idle":"2022-04-08T17:55:25.615076Z","shell.execute_reply.started":"2022-04-08T17:55:25.605082Z","shell.execute_reply":"2022-04-08T17:55:25.614251Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"decoder=Decoder(embedding_dim, units, vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:28.361459Z","iopub.execute_input":"2022-04-08T17:55:28.361863Z","iopub.status.idle":"2022-04-08T17:55:28.390334Z","shell.execute_reply.started":"2022-04-08T17:55:28.361815Z","shell.execute_reply":"2022-04-08T17:55:28.389679Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"features=encoder(sample_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:35.555110Z","iopub.execute_input":"2022-04-08T17:55:35.555717Z","iopub.status.idle":"2022-04-08T17:55:35.821246Z","shell.execute_reply.started":"2022-04-08T17:55:35.555677Z","shell.execute_reply":"2022-04-08T17:55:35.820433Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-8\"></a>\n# Model training & optimization\n\n1. Set the optimizer & loss object\n2. Create the checkpoint path\n3. Create the training & testing step functions\n4. Create the loss function for the test dataset","metadata":{}},{"cell_type":"code","source":"LR = 2e-4\noptimizer = tf.keras.optimizers.Adam(learning_rate = LR)         #define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction='none')   #define the loss object","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:41.297481Z","iopub.execute_input":"2022-04-08T17:55:41.297817Z","iopub.status.idle":"2022-04-08T17:55:41.303397Z","shell.execute_reply.started":"2022-04-08T17:55:41.297773Z","shell.execute_reply":"2022-04-08T17:55:41.302605Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:43.581198Z","iopub.execute_input":"2022-04-08T17:55:43.581512Z","iopub.status.idle":"2022-04-08T17:55:43.587340Z","shell.execute_reply.started":"2022-04-08T17:55:43.581477Z","shell.execute_reply":"2022-04-08T17:55:43.585798Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = os.path.join(working_folder_path,'checkpoint')\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:46.461959Z","iopub.execute_input":"2022-04-08T17:55:46.462830Z","iopub.status.idle":"2022-04-08T17:55:46.470369Z","shell.execute_reply.started":"2022-04-08T17:55:46.462779Z","shell.execute_reply":"2022-04-08T17:55:46.469551Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:48.949246Z","iopub.execute_input":"2022-04-08T17:55:48.949802Z","iopub.status.idle":"2022-04-08T17:55:48.954414Z","shell.execute_reply.started":"2022-04-08T17:55:48.949759Z","shell.execute_reply":"2022-04-08T17:55:48.953144Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### * While creating the training step for our model, we apply Teacher forcing.\n#### * Teacher forcing is a technique where the target/real word is passed as the next input to the decoder instead of previous prediciton.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n            \n    avg_loss = (loss/int(target.shape[1]))\n    \n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, trainable_variables)\n    optimizer.apply_gradients(zip(gradients, trainable_variables))     \n                       \n    return loss, avg_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:56.056408Z","iopub.execute_input":"2022-04-08T17:55:56.056679Z","iopub.status.idle":"2022-04-08T17:55:56.066963Z","shell.execute_reply.started":"2022-04-08T17:55:56.056647Z","shell.execute_reply":"2022-04-08T17:55:56.066232Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### * While creating the test step for our model, we pass oour previous prediciton as the next input to the decoder.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n    avg_loss = (loss / int(target.shape[1]))\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, trainable_variables)\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    \n    return loss, avg_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:55:59.788158Z","iopub.execute_input":"2022-04-08T17:55:59.788440Z","iopub.status.idle":"2022-04-08T17:55:59.797821Z","shell.execute_reply.started":"2022-04-08T17:55:59.788407Z","shell.execute_reply":"2022-04-08T17:55:59.796566Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def test_loss_cal(test_dataset):\n    total_loss = 0\n    \n    # Code to get the average loss result on our test data\n    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n    avg_test_loss=total_loss/test_num_steps\n    \n    return avg_test_loss","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:56:03.119740Z","iopub.execute_input":"2022-04-08T17:56:03.120278Z","iopub.status.idle":"2022-04-08T17:56:03.124549Z","shell.execute_reply.started":"2022-04-08T17:56:03.120238Z","shell.execute_reply":"2022-04-08T17:56:03.123901Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss / train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T17:56:10.738133Z","iopub.execute_input":"2022-04-08T17:56:10.738926Z","iopub.status.idle":"2022-04-08T18:26:50.343874Z","shell.execute_reply.started":"2022-04-08T17:56:10.738884Z","shell.execute_reply":"2022-04-08T18:26:50.343151Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Final Test loss after 15 epochs = 0.611","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training & Test loss plot wrt number of epochs')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:26:57.907647Z","iopub.execute_input":"2022-04-08T18:26:57.908145Z","iopub.status.idle":"2022-04-08T18:26:58.123345Z","shell.execute_reply.started":"2022-04-08T18:26:57.908110Z","shell.execute_reply":"2022-04-08T18:26:58.122611Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: \n* Since there is a difference between the train & test steps ( Presence of teacher forcing), we may observe that the train loss is decreasing while your test loss is not. \n* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n* Also, if we want to achieve better results we can run it more epochs, but the intent of this project is to get an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-9\"></a>\n# Model Evaluation\n\n1. Define the evaluation function using greedy search\n2. Define the evaluation function using beam search\n3. Test it on a sample data using BLEU score","metadata":{}},{"cell_type":"markdown","source":"### Greedy Search","metadata":{}},{"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length_caption, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)  # process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input)        # Extracting features using feature extraction model\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)       # extracting the features by passing the input to encoder\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length_caption):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)    # getting the output from decoder\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n        \n        # extract the predicted id(embedded value) which carries the max value\n        predicted_id = tf.argmax(predictions[0]).numpy()     \n        # map the id to the word from tokenizer and append the value to the result list\n        result.append(tokenizer.index_word[predicted_id])\n        \n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:07.330375Z","iopub.execute_input":"2022-04-08T18:27:07.330961Z","iopub.status.idle":"2022-04-08T18:27:07.339885Z","shell.execute_reply.started":"2022-04-08T18:27:07.330920Z","shell.execute_reply":"2022-04-08T18:27:07.339036Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Beam Search","metadata":{}},{"cell_type":"code","source":"def beam_evaluate(image, beam_index = 3):#your value for beam index):\n    \n    # to evaluate the result using beam search\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    attention_plot = np.zeros((max_length_caption, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n\n    while len(result[0][0]) < max_length_caption:\n        i=0\n        temp = []\n        for s in result:\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:]\n          \n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n            \n                prob += np.log(predictions[0][w])\n                    \n                temp.append([next_cap, prob])\n        result = temp\n        result = sorted(result, reverse=False, key=lambda l: l[1])\n        result = result[-beam_index:]\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3):\n            dec_input = tf.expand_dims([prd_id], 0)  \n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    \n    return final_caption\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:13.243198Z","iopub.execute_input":"2022-04-08T18:27:13.243955Z","iopub.status.idle":"2022-04-08T18:27:13.257714Z","shell.execute_reply.started":"2022-04-08T18:27:13.243911Z","shell.execute_reply":"2022-04-08T18:27:13.256905Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def plot_attention_map(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(weights[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:18.318461Z","iopub.execute_input":"2022-04-08T18:27:18.318742Z","iopub.status.idle":"2022-04-08T18:27:18.327926Z","shell.execute_reply.started":"2022-04-08T18:27:18.318711Z","shell.execute_reply":"2022-04-08T18:27:18.327127Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:19.517436Z","iopub.execute_input":"2022-04-08T18:27:19.518049Z","iopub.status.idle":"2022-04-08T18:27:19.523489Z","shell.execute_reply.started":"2022-04-08T18:27:19.518007Z","shell.execute_reply":"2022-04-08T18:27:19.522562Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def caption_prediction(autoplay=False, weights=(0.5, 0.5, 0, 0)):\n    rid = np.random.randint(0, len(path_test))\n    test_image = path_test[rid]\n    #test_image = './images/413231421_43833a11f5.jpg'\n    #real_caption = '<start> black dog is digging in the snow <end>'\n\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n    result, attention_plot, pred_test = evaluate(test_image)\n    \n\n    real_caption=filt_text(real_caption)      \n\n    \n    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = pred_caption.split()\n    \n    score = sentence_bleu(reference, candidate, weights = weights)     #set your weights)\n    print(f\"BELU score: {score*100}\")\n    \n    print('Real Caption:', real_caption)\n    print(f\"Prediction Caption: {pred_caption}\")\n    plot_attention_map(result, attention_plot, test_image)\n    \n    # we make use of Google Text to Speech API (online), which will convert the caption to audio\n    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n    speech.save('voice.mp3')\n    audio_file = 'voice.mp3'\n\n    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n    \n    return Image.open(test_image)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:25.188917Z","iopub.execute_input":"2022-04-08T18:27:25.189209Z","iopub.status.idle":"2022-04-08T18:27:25.201535Z","shell.execute_reply.started":"2022-04-08T18:27:25.189174Z","shell.execute_reply":"2022-04-08T18:27:25.200727Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 1","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.5, 0.5, 0, 0))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:27:46.924282Z","iopub.execute_input":"2022-04-08T18:27:46.924546Z","iopub.status.idle":"2022-04-08T18:27:48.371559Z","shell.execute_reply.started":"2022-04-08T18:27:46.924515Z","shell.execute_reply":"2022-04-08T18:27:48.370811Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 2","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.25, 0.25, 0, 0))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:28:01.218604Z","iopub.execute_input":"2022-04-08T18:28:01.219074Z","iopub.status.idle":"2022-04-08T18:28:02.476358Z","shell.execute_reply.started":"2022-04-08T18:28:01.219029Z","shell.execute_reply":"2022-04-08T18:28:02.475515Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 3","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.2, 0.3, 0.4, 0.5))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:28:10.605829Z","iopub.execute_input":"2022-04-08T18:28:10.606634Z","iopub.status.idle":"2022-04-08T18:28:11.767366Z","shell.execute_reply.started":"2022-04-08T18:28:10.606585Z","shell.execute_reply":"2022-04-08T18:28:11.766663Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 4","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.5, 0.4, 0.3, 0.2))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:28:37.333125Z","iopub.execute_input":"2022-04-08T18:28:37.333620Z","iopub.status.idle":"2022-04-08T18:28:38.388515Z","shell.execute_reply.started":"2022-04-08T18:28:37.333566Z","shell.execute_reply":"2022-04-08T18:28:38.387406Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 5","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.25, 0.25, 0.25, 0.25))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:28:48.495552Z","iopub.execute_input":"2022-04-08T18:28:48.495837Z","iopub.status.idle":"2022-04-08T18:28:49.732193Z","shell.execute_reply.started":"2022-04-08T18:28:48.495804Z","shell.execute_reply":"2022-04-08T18:28:49.731612Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### Testing image - 6","metadata":{}},{"cell_type":"code","source":"caption_prediction(autoplay=True, weights=(0.3, 0.3, 0.4, 0.4))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:29:01.098878Z","iopub.execute_input":"2022-04-08T18:29:01.099235Z","iopub.status.idle":"2022-04-08T18:29:02.373066Z","shell.execute_reply.started":"2022-04-08T18:29:01.099193Z","shell.execute_reply":"2022-04-08T18:29:02.372285Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-10\"></a>\n# Conclusions","metadata":{}},{"cell_type":"markdown","source":"#### * Inception V3 Model with pretrained weights (Imagenet)was used to extract feature vectors\n#### * Attention model is used to extract context vectors before feeding into decoder/RNN\n#### * Testing was carried out with different combinations of weights","metadata":{}}]}